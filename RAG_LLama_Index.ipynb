{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install weaviate-client tiktoken pypdf rapidocr-onnxruntime\n",
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZvw7IzhQlew",
        "outputId": "57cc2639-f7f1-4039-b9ea-9bde0880e94e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.0)\n",
            "Requirement already satisfied: rapidocr-onnxruntime in /usr/local/lib/python3.11/dist-packages (1.4.4)\n",
            "Requirement already satisfied: requests<2.28.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (2.27.1)\n",
            "Requirement already satisfied: validators<0.19.0,>=0.18.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (0.18.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.59.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: pyclipper>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (1.3.0.post6)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (4.11.0.86)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (2.0.7)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (6.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (11.1.0)\n",
            "Requirement already satisfied: onnxruntime>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (25.1.24)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<2.28.0,>=2.23.0->weaviate-client) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<2.28.0,>=2.23.0->weaviate-client) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests<2.28.0,>=2.23.0->weaviate-client) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<2.28.0,>=2.23.0->weaviate-client) (3.10)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from validators<0.19.0,>=0.18.2->weaviate-client) (4.4.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.17)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.35)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.18 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.18)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.27.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.7.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.6)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall weaviate-client -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nr3k-AoYnAy",
        "outputId": "5ce028d0-9b42-48df-820b-74b9c3b7545b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: weaviate-client 3.5.0\n",
            "Uninstalling weaviate-client-3.5.0:\n",
            "  Successfully uninstalled weaviate-client-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install weaviate-client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "v7BOInkYYpYJ",
        "outputId": "32d0f257-60e7-49cd-ac5b-31b017a9c8eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weaviate-client\n",
            "  Using cached weaviate_client-4.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (0.28.1)\n",
            "Collecting validators==0.34.0 (from weaviate-client)\n",
            "  Using cached validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: authlib<1.3.2,>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (2.10.6)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.70.0)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.70.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.70.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib<1.3.2,>=1.2.1->weaviate-client) (43.0.3)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.11/dist-packages (from grpcio-health-checking<2.0.0,>=1.66.2->weaviate-client) (5.29.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (4.12.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (2.22)\n",
            "Using cached weaviate_client-4.10.4-py3-none-any.whl (330 kB)\n",
            "Using cached validators-0.34.0-py3-none-any.whl (43 kB)\n",
            "Installing collected packages: validators, weaviate-client\n",
            "  Attempting uninstall: validators\n",
            "    Found existing installation: validators 0.18.2\n",
            "    Uninstalling validators-0.18.2:\n",
            "      Successfully uninstalled validators-0.18.2\n",
            "Successfully installed validators-0.34.0 weaviate-client-4.10.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "validators",
                  "weaviate"
                ]
              },
              "id": "0daf4e30e9f644af8677826718d36b60"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_API_KEY = \"FQEl8qmgPo4o7zgABwLGmVFeoFn2BRKhqB9j\"\n",
        "WEAVIATE_CLUSTER_ENDPOINT = \"https://l3mgo1crsnc9adex7yenq.c0.us-east1.gcp.weaviate.cloud\""
      ],
      "metadata": {
        "id": "MCGqAOZgQyjz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate\n",
        "\n",
        "WEAVIATE_URL = WEAVIATE_CLUSTER_ENDPOINT\n",
        "WEAVIATE_API_KEY = WEAVIATE_API_KEY\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WEAVIATE_URL, auth_credentials=weaviate.AuthApiKey(WEAVIATE_API_KEY)\n",
        ")"
      ],
      "metadata": {
        "id": "kQQVNo6XRDm7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_mode_name = \"all-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device': 'cuda'}\n",
        "embedding = HuggingFaceEmbeddings(model_name=embedding_mode_name, model_kwargs = model_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "col1X9PeSkoa",
        "outputId": "9e4988c6-f8e3-495c-b220-f95eafba3626"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-feac86a9a53d>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings(model_name=embedding_mode_name, model_kwargs = model_kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def list_pdf_files(directory):\n",
        "    pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
        "    return [str(pdf) for pdf in pdf_files]\n",
        "\n",
        "# Example usage\n",
        "directory_path = \"./\"  # Change this to your directory path\n",
        "pdf_files = list_pdf_files(directory_path)\n",
        "\n",
        "print(pdf_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwLPn8meT9kK",
        "outputId": "334966e7-ac85-48ae-dbef-af7156a69bc2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lecture 4 - Perceptron.pdf', 'Lecture 8 - Linear Regression (corrected).pdf', 'Lecture 7 - Logistic Regression.pdf', 'Lecture 1 - Supervised Learning.pdf', 'Lecture 10 - Kernels.pdf', 'Lecture 11 - Kernels.pdf', 'Lecture 2 - k Nearest Neighbors.pdf', 'Lecture 5 & 6 - Gradient Descent (corrected).pdf', 'Lecture 9 - SVMs.pdf', 'Lecture 2 - Supervised Learning.pdf', 'Lecture 3 - k Nearest Neighbors.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "all_pages = []\n",
        "for files in pdf_files:\n",
        "    loader = PyPDFLoader(files, extract_images=True)\n",
        "    pages = loader.load()\n",
        "    all_pages.extend(pages)\n",
        "\n",
        "print(f\"Total pages read: {len(all_pages)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKd3s5lvTR9i",
        "outputId": "b31a4df4-e672-4f79-bccc-4794efd2bdc4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages read: 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# character splitter\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "docs = text_splitter.split_documents(all_pages)"
      ],
      "metadata": {
        "id": "fneC8nvSUDki"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    doc.metadata = {k.replace(\".\", \"_\"): v for k, v in doc.metadata.items()}\n"
      ],
      "metadata": {
        "id": "j8AYTheNcKYD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
        "db = WeaviateVectorStore.from_documents(docs, embedding, client=client)\n",
        "\n"
      ],
      "metadata": {
        "id": "pERnM_bGWk6j"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity search\n",
        "out = db.similarity_search(\"what are the parameters in gradient descent algorithm\", k = 3)"
      ],
      "metadata": {
        "id": "eI4kyCoyb3wy"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in out:\n",
        "    print(i.page_content)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZipMB2qcs6b",
        "outputId": "44737d8e-6b9f-40d3-bcd7-1f3212865dfe"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2v⊤Hv\n",
            "\u0015\n",
            "= ∇F(w) + Hv\n",
            "∇F(w) + Hv = 0 ⇒ v = −H−1∇F(w)\n",
            "In settings where the quadratic approximation is accurate (this is the same question we had for\n",
            "standard gradient descent!), this update rule ( wt+1 ← wt − H−1∇F(w)) converges extremely fast.\n",
            "Interestingly, note the lack of step size in standard Newton’s method: this is a common source of\n",
            "divergence. If there are directions of the function that are extremely shallow, the inverse Hessian\n",
            "will have extremely large eigenvalues, leading to very large steps in those directions.\n",
            "2 When is gradient descent good?\n",
            "In principle, the recipe for using gradient descent in machine learning is pretty simple: specify a\n",
            "bunch of parameters that define a hypothesis class H, pick a loss function that specifies how bad\n",
            "3\n",
            "\n",
            "F(w + v) ≈ F(w) + ∇F(w)⊤v\n",
            "F(w + v) ≈ F(w) + ∇F(w)⊤v + 1\n",
            "2v⊤Hv\n",
            "Here, ∇F(w) is the gradient and H is the Hessian, e.g. the matrix so that Hij = ∂2F\n",
            "∂wi∂wj\n",
            ".\n",
            "1.1 Gradient Descent\n",
            "The Taylor expansion gives us a nice approximation toF(w) that lets us reason locally about how to\n",
            "decrease the objective. How do we use these approximations? The fundamental idea is, given some\n",
            "current wt, we will use this approximation to choose a reasonable v so that F(w + v) < F(w), and\n",
            "therefore we can updatewt+1 = wt−ηtv and get a better model h. Consider the following algorithm:\n",
            "Algorithm 1: Gradient Descent (GD)\n",
            "Initialize w1 ∈ Rd\n",
            "while t = 1, 2, . . . , Tdo\n",
            "Update wt+1 = wt − ηt∇F(wt)\n",
            "end\n",
            "At each iteration, the algorithm computes the gradient of F(·) at the current location wt, and\n",
            "then simply moves a little bit in the direction of the negative gradient. Here ηt are known as the\n",
            "learning rates. Usually we use a stopping condition to terminate the algorithm, for instance, when\n",
            "\n",
            "smaller than that.\n",
            "In modern machine learning, several very crazy learning rate schedules work well, counter-intuitive\n",
            "to what we can get guarantees for in convex problems. This is a very active area of research.\n",
            "It’s important to wonder at this point what conditions might be useful for gradient descent working\n",
            "quickly or even at all, and we will cover an introduction to this.\n",
            "1.2 Newton’s method\n",
            "In Newton’s method, we make use of the second order Taylor expansion of the function,\n",
            "F(w + v) ≈ F(w) + ∇F(w)⊤v + 1\n",
            "2v⊤Hv\n",
            "Like in gradient descent, the fundamental question is how to use this approximation to choose a\n",
            "reasonable v. Unlike in gradient descent where we approximated the function with a line that gives\n",
            "us a direction to follow, here we get a quadratic function that we can minimize directly and find\n",
            "the best v under this approximation . To do this, we can take the derivative of the approximation\n",
            "with respect to v, set it equal to zero and solve:\n",
            "∇v\n",
            "\u0014\n",
            "F(w) + ∇F(w)⊤v + 1\n",
            "2v⊤Hv\n",
            "\u0015\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "qhWqlU1lc4DU"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "L-_LfB4ydb1K"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNv5SEqmdcYL",
        "outputId": "d8f4eb8d-9dbc-4810-ef4e-90f6296230eb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "from google.colab import userdata\n",
        "# The gemini api key\n",
        "API_KEY = userdata.get('hf')\n",
        "print(API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwaA3kNPddeS",
        "outputId": "c8d9043e-6309-417a-f795-ede5dcfd8457"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=API_KEY,\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    model_kwargs={\"temperature\":1, \"max_length\":180}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB9W5HUddfuT",
        "outputId": "ef58af4f-22d0-4bf7-cf9f-0efeac5278a0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-b05aafd27aab>:1: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  model = HuggingFaceHub(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "WiZsoMQCeCJD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser=StrOutputParser()\n"
      ],
      "metadata": {
        "id": "CUAREONNe33T"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=db.as_retriever()\n"
      ],
      "metadata": {
        "id": "kxskHdXze_6b"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "x9CD0AqgfB4a"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"what are the parameters in gradient descent algorithm?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObWCPYC3fC9l",
        "outputId": "86253a6d-1bf8-426e-e1e6-aa9c2f322498"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: what are the parameters in gradient descent algorithm?\n",
            "Context: [Document(metadata={'subject': '', 'page_label': '3', 'total_pages': 9.0, 'creator': 'LaTeX with hyperref', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 2, 7, 18, 26, 2, tzinfo=datetime.timezone.utc), 'source': 'Lecture 5 & 6 - Gradient Descent (corrected).pdf', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': datetime.datetime(2024, 2, 7, 18, 26, 2, tzinfo=datetime.timezone.utc), 'producer': 'pdfTeX-1.40.25', 'page': 2.0, 'title': '', 'author': ''}, page_content='2v⊤Hv\\n\\x15\\n= ∇F(w) + Hv\\n∇F(w) + Hv = 0 ⇒ v = −H−1∇F(w)\\nIn settings where the quadratic approximation is accurate (this is the same question we had for\\nstandard gradient descent!), this update rule ( wt+1 ← wt − H−1∇F(w)) converges extremely fast.\\nInterestingly, note the lack of step size in standard Newton’s method: this is a common source of\\ndivergence. If there are directions of the function that are extremely shallow, the inverse Hessian\\nwill have extremely large eigenvalues, leading to very large steps in those directions.\\n2 When is gradient descent good?\\nIn principle, the recipe for using gradient descent in machine learning is pretty simple: specify a\\nbunch of parameters that define a hypothesis class H, pick a loss function that specifies how bad\\n3'), Document(metadata={'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 9.0, 'page_label': '2', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 2, 7, 18, 26, 2, tzinfo=datetime.timezone.utc), 'moddate': datetime.datetime(2024, 2, 7, 18, 26, 2, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'Lecture 5 & 6 - Gradient Descent (corrected).pdf', 'title': '', 'page': 1.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='F(w + v) ≈ F(w) + ∇F(w)⊤v\\nF(w + v) ≈ F(w) + ∇F(w)⊤v + 1\\n2v⊤Hv\\nHere, ∇F(w) is the gradient and H is the Hessian, e.g. the matrix so that Hij = ∂2F\\n∂wi∂wj\\n.\\n1.1 Gradient Descent\\nThe Taylor expansion gives us a nice approximation toF(w) that lets us reason locally about how to\\ndecrease the objective. How do we use these approximations? The fundamental idea is, given some\\ncurrent wt, we will use this approximation to choose a reasonable v so that F(w + v) < F(w), and\\ntherefore we can updatewt+1 = wt−ηtv and get a better model h. Consider the following algorithm:\\nAlgorithm 1: Gradient Descent (GD)\\nInitialize w1 ∈ Rd\\nwhile t = 1, 2, . . . , Tdo\\nUpdate wt+1 = wt − ηt∇F(wt)\\nend\\nAt each iteration, the algorithm computes the gradient of F(·) at the current location wt, and\\nthen simply moves a little bit in the direction of the negative gradient. Here ηt are known as the\\nlearning rates. Usually we use a stopping condition to terminate the algorithm, for instance, when'), Document(metadata={'subject': '', 'page_label': '3', 'total_pages': 9.0, 'creator': 'LaTeX with hyperref', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 2, 7, 18, 26, 2, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'Lecture 5 & 6 - Gradient Descent (corrected).pdf', 'moddate': datetime.datetime(2024, 2, 7, 18, 26, 2, tzinfo=datetime.timezone.utc), 'title': '', 'page': 2.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='smaller than that.\\nIn modern machine learning, several very crazy learning rate schedules work well, counter-intuitive\\nto what we can get guarantees for in convex problems. This is a very active area of research.\\nIt’s important to wonder at this point what conditions might be useful for gradient descent working\\nquickly or even at all, and we will cover an introduction to this.\\n1.2 Newton’s method\\nIn Newton’s method, we make use of the second order Taylor expansion of the function,\\nF(w + v) ≈ F(w) + ∇F(w)⊤v + 1\\n2v⊤Hv\\nLike in gradient descent, the fundamental question is how to use this approximation to choose a\\nreasonable v. Unlike in gradient descent where we approximated the function with a line that gives\\nus a direction to follow, here we get a quadratic function that we can minimize directly and find\\nthe best v under this approximation . To do this, we can take the derivative of the approximation\\nwith respect to v, set it equal to zero and solve:\\n∇v\\n\\x14\\nF(w) + ∇F(w)⊤v + 1\\n2v⊤Hv\\n\\x15'), Document(metadata={'subject': '', 'page_label': '4', 'total_pages': 9.0, 'creator': 'LaTeX with hyperref', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 2, 7, 18, 26, 2, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'Lecture 5 & 6 - Gradient Descent (corrected).pdf', 'moddate': datetime.datetime(2024, 2, 7, 18, 26, 2, tzinfo=datetime.timezone.utc), 'title': '', 'page': 3.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='for a long time it was thought in machine learning that these properties were very important to\\nhave, and many classic machine learning algorithms that we’ll learn satisfy them. In particular,\\nthere are two properties we will study that make gradient descent extra nice:\\n1. Smoothness.\\n2. (Strong) convexity.\\n3 Convex functions and sets\\nThe first interesting properties that our functionF(w) and constraint setC can satisfy is convexity.\\nAs we’ll see, one of the key things that convexity will guarantee is that, if gradient descent finds a\\nstationary point (e.g., a point w so that ∇F(w) = 0), that stationary point is guaranteed to be a\\nglobal optimum! In addition, a function can be strongly convex, which (along with smoothness)\\nwill result in very fast convergence.\\nDefinition 1 (Convex function). A function F : Rd → R is convex if for all w, w′ ∈ Rd and\\nα ∈ [0, 1],\\nF\\n\\x00\\nαw + (1 − α)w′\\x01\\n≤ αF(w) + (1− α)F(w′).')]\n",
            "Answer:\n",
            "The parameters in the gradient descent algorithm are:\n",
            "- **Learning Rate (η)**: Controls the step size in each iteration.\n",
            "- **Gradient (∇F(w))**: The direction of the steepest ascent of the function F(w).\n",
            "- **Iteration Number (t)**: Keeps track of the current step in the algorithm.\n",
            "- **Number of Iterations (T)**: The total number of steps the algorithm will take before stopping.\n",
            "- **Initial Parameters (w1)**: The starting point for the algorithm.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(rag_chain.invoke(\"what is perceptron?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8_h7If9fD26",
        "outputId": "c050a4a3-d73e-4023-b067-09379fc3a99d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: what is perceptron?\n",
            "Context: [Document(metadata={'subject': '', 'page_label': '2', 'total_pages': 6.0, 'creator': 'LaTeX with hyperref', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 30, 22, 39, 38, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': datetime.datetime(2024, 1, 30, 22, 39, 38, tzinfo=datetime.timezone.utc), 'source': 'Lecture 4 - Perceptron.pdf', 'title': '', 'page': 1.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='comprises neural networks today, albeit with different ”activation functions” than the sign/step\\nfunction. The term MLP, common jargon in deep learning, stands for Multi-layer Perceptron.\\nIn 1957, Frank Rosenblatt came up with an algorithm called ”Perceptron” to learn this model from\\ndata. The invention of the algorithm gained wide popularity and created a huge wave of excite-\\n2'), Document(metadata={'subject': '', 'page_label': '3', 'total_pages': 6.0, 'creator': 'LaTeX with hyperref', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 30, 22, 39, 38, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'Lecture 4 - Perceptron.pdf', 'moddate': datetime.datetime(2024, 1, 30, 22, 39, 38, tzinfo=datetime.timezone.utc), 'title': '', 'page': 2.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='Figure 2: Frank Rosenblatt with a Mark I Perceptron computer in 1960.\\nment. Check out this article from the New York Times titled Electronic ‘Brain’ Teaches Itself in\\n1958 ( https://www.nytimes.com/1958/07/13/archives/electronic-brain-teaches-itself.\\nhtml). Perhaps, this was the first instance of what we call deep learning now.\\n3.1 Algorithm\\nWe will now see how the Perceptron algorithm (Algorithm 1) solves the ERM problem in the\\nlinearly separable case. The Perceptron algorithm can also be run with data in an online fashion,\\nbut we will discuss the batch version here.\\nAlgorithm 1:Perceptron\\nInitialize w1 = 0 ∈ Rd\\nfor t = 1, 2, . . .do\\nif ∃i ∈ {1, . . . , m} s.t. yi ̸= sign\\n\\x00\\nw⊤\\nt xi\\n\\x01\\nthen update wt+1 = wt + yixi\\nelse output wt\\nend\\nThe update may seem strange, but let us see the intuition behind this. Suppose xi is the example\\nthat is misclassified and assume ∥xi∥2 = 1.\\n• If the true label was positive, then w⊤\\nt+1xi = w⊤\\nt xi + ∥xi∥2 = w⊤\\nt xi + 1.'), Document(metadata={'subject': '', 'page_label': '1', 'total_pages': 8.0, 'creator': 'LaTeX with hyperref', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 2, 15, 5, 34, 8, tzinfo=datetime.timezone.utc), 'source': 'Lecture 9 - SVMs.pdf', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': datetime.datetime(2024, 2, 15, 5, 34, 8, tzinfo=datetime.timezone.utc), 'title': '', 'page': 0.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='that there are orange circles very close to the green hyperplane – it’s not implausible to think that\\nthere could be orange circle test points just to the left of the given training data that we would\\nclassify incorrectly. Put another way, the purple hyperplane is better because it seems to “leave\\nmore room for error.” On the right, we see that we can formalize this intuition using a quantity\\nwe’ve already seen before: the margin. The purple hyperplane is “better” because it has a larger\\nmargin. SVMs are linear models with the maximum possible margin given a dataset.\\nNote: Recall that the Perceptron algorithm guaranteed to find a hyperplane that separates the data\\nas long as one exists. In fact, since we assumed separation with margin, there are infinitely many\\nhyperplanes that separate the data, and the Perceptron may find any one of these. In particular,\\nPerceptron algroithm did not provide any guarantees on the margin of the hyperplane found, even'), Document(metadata={'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 6.0, 'page_label': '6', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 30, 22, 39, 38, tzinfo=datetime.timezone.utc), 'moddate': datetime.datetime(2024, 1, 30, 22, 39, 38, tzinfo=datetime.timezone.utc), 'source': 'Lecture 4 - Perceptron.pdf', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'producer': 'pdfTeX-1.40.25', 'page': 5.0, 'title': '', 'author': ''}, page_content='Figure 4: Data drawn from an XOR function. Source: Lecture notes from Cornell CS 4/5780\\nhttps: // www. cs. cornell. edu/ courses/ cs4780/ 2022sp/ notes/ Notes06. pdf.\\nLimitations of Perceptron. Minksy and Papert wrote a book titled ’Perceptrons’ in 1969 which\\nshowed that the Perceptron algorithm could not learn XOR functions (Figure 4). Such data is not\\nlinearly separable.\\nThe book became widely popular as a criticism for neural networks (or perceptrons) and is said to\\nhave led to the AI winter which lasted till the mid-90s.\\nApart from not working on non-linearly separable data, it cannot handle noise in the label. This\\nhas led to developments of other techniques that we will study in the later part of the course.\\n6\\n\\n\\n\\nX\\nX\\n×\\n×')]\n",
            "Answer:\n",
            "The Perceptron is a supervised learning algorithm introduced by Frank Rosenblatt in 1957. It's a type of binary classifier that uses a linear function to make predictions. The Perceptron algorithm works by adjusting a weight vector based on misclassified examples, aiming to find a hyperplane that separates the data. It's simple, efficient, and was a significant early contribution to the field of machine learning. However, it has limitations, such as not being able to learn from non-linearly separable data and being sensitive to noise in labels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8KJ8kUVffLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}